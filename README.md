# GP_2
## Описание задания
В современном мире данных эффективный сбор и интеграция информации из различных источников являются фундаментом для принятия обоснованных решений и успешного ведения бизнеса. Компании, стремясь к конкурентоспособности, активно используют технологии веб-скрапинга и взаимодействия с API для автоматизации процесса сбора данных, обеспечения их актуальности и полноты.

В данном проекте вы на реальных данных попрактикуетесь в том, как осуществляется веб-скрапинг различных сайтов для извлечения необходимой информации, а также в работе с API популярных сервисов для получения структурированных данных. Вы научитесь применять современные инструменты и библиотеки, отражающие различные этапы DE, обрабатывать и очищать полученную информацию, а также интегрировать данные из различных источников для последующего использования их в аналитических и бизнес-задачах.


### Постановка задачи
В рамках этого проекта вы сами найдете датасет, с которым будете работать. Точнее, не найдете, а... соберёте его самостоятельно на основе источников!

Итак, начните с того, что выберите два сайта/портала со схожей тематикой (например, NYT и Bloomberg). Их тематика должна позволять в дальнейшем объединить данные в единую таблицу. Один из этих двух сайтов нужно будет обработать с помощью средств Web-Scraping'a и Data Parsing'a; другой же — используя API.

Определившись с источниками, непосредственно проделайте скачивание данных путём скрейпинга и методами API соответственно и затем объедините всё в финальный датасет. Обязательным требованием к датасету является также наличие в нём как минимум одного столбца с текстовыми данными: описание, состав, комментарии о чём-либо, текст заметки/новости, или что-то ещё.

Кроме того, разумеется, следует помнить, что полученный датасет должен, конечно, быть осмысленным с бизнес-точки зрения и потенциально позволять решать какую-то конкретную бизнес-задачу. Это как минимум означает, что каждому объекту, имеющему своё признаковое пространство — в том числе с текстовым признаком, — должно также соответствовать значение некоторой целевой переменной, которая является релевантной в рамках выбранной вами предметной области.


Примеры задач, которые мы можем предложить вам в качестве источника для вдохновения — пожалуйста, ни в коем случае не воспринимайте это как исчерпывающий список — это действительно всего лишь примеры:

* Сайт новостей: текстовый столбец — сама новость; признаки — различные характеристики новости: автор, дата публикации, тип новости и т.д.; целевая переменная — количество просмотров новости (можно поделить на число дней с момента даты публикации, чтобы получить «среднее число просмотров в день», что ещё нагляднее)

* Сайт с товарами/книгами/фильмами: текстовый столбец — описание товара/книги/фильма; признаки — различные характеристики объекта: цена, год производства/выпуска, компания-производитель и т.д.; целевая переменная — средний рейтинг объекта

* Блоги: текстовый столбец — тексты заметок; признаки — различные характеристики заметки: автор, время публикации, количество прикрепленных изображений и т.д.; целевая переменная — число просмотров

* И любые другие ваши идеи, которые подходят под описанный выше формат

После сбора данных и объединения их в общий датасет, необходимо очистить его от мусора (в том числе, например, с помощью регулярных выражений); посчитать различные статистики по собранным данным (в случае если столбец текстовый, то, например, посчитать частотности слов, выявляя наиболее частотные слова, и т.д.); а также в целом провести EDA в контексте той бизнес-задачи, которую мы потенциально хотим решить по полученному датасету. В конце сделать выводы из проделанной работы.

### Этапы проекта
Ниже для вашего удобства еще раз в краткой форме представляем примерные этапы реализации проекта:

- Выбор источников данных, в соответствии с требованиями, описанными выше
- Формулировка бизнес-задачи, которую могла бы хотеть решить компания/платформа, на основе собираемых данных
- Обоснование сформулированной бизнес-задачи
- Извлечение данных путём работы с API
- Извлечение данных путём скрапинга и парсинга
- Первичная обработка данных (EDA)
- Получение и агрегация результатов, финальные выводы


### Общие требования к проекту
В качестве результата выполнения группового проекта ваша команда должна получить презентацию, а также репозиторий с кодом на GitHub. Демонстрация репозитория и защита презентации перед комиссией осуществляется в фиксированную дату.

Оформление презентации остаётся полностью на ваше усмотрение, но помните, что результат должен быть релевантен для демонстрации бизнес-заказчику — комиссию, принимающую вашу работу, правильнее всего воспринимать именно в таком качестве. Например, вставлять в презентацию строчки кода или злоупотреблять скринами блокнота не рекомендуется.

С точки зрения концепции выполнения проекта — вам необходимо принять на себя роль дата-инженеров: продумать идеологию получения и извлечения данных с платформы — и дата-аналитиков: соотнести бизнес-задачу с формулировками на языке данных и извлечь полезные инсайты из проделанной работы. А в конце, конечно, — презентовать всю эту красоту в понятном виде потенциальному заказчику.

Разумеется, чем сложнее и чем нетривиальнее будет ваша задача, а также чем глубже и чем осмысленнее будут выводы, извлечённые в рамках этой задачи, — тем выше окажется качество вашего проекта и тем выше он будет оценён!

#### Обязательный сбор данных через API
* Необходимо реализовать сбор данных через API для как минимум одного веб-сайта
* При формировании датасета должно осуществляться **не менее 5** различных запросов к API
* Данный этап является обязательным, что означает, что без его полного выполнения ***нельзя получить оценку за проект выше 1***

#### Обязательный сбор данных посредством скрапинга
* Необходимо реализовать сбор и парсинг данных через библиотеки `requests` / `bs4` и другие рассмотренные в рамках семинара для как минимум одного веб-сайта, отличного от веб-сайта из предыдущего пункта
* Получившийся датасет должен содержать **не менее 10 000** записей
* Данный этап является обязательным, что означает, что без его полного выполнения ***нельзя получить оценку за проект выше 1***

#### Разведочный анализ данных (EDA)
* Изучить особенности данных в рамках объединённого датасета:
  * Есть ли пропущенные значения? Сколько? Какова их доля от общего числа объектов для каждого конкретного признака? И, в целом, насколько ваш датасет “разрежен” в контексте пропущенных значений? Выдвинуть и (желательно) проверить гипотезу, что могут означать пропущенные значения? Осмысленно и аргументированно обработать пропуски.
  * Есть ли ошибочные  (например, отрицательный возраст или пятиметровый рост человека; 3-й класс в задаче бинарной классификации) значения (признаки, целевая переменная) в данных? Обработать их.
  * Есть ли выбросы в данных? По возможности обработать их.
* Изучить поведение каждого признака в отдельности, его связь с целевой переменной, а также попарные корреляции (в том числе и в особенности с таргетом). Визуализировать попарные распределения признаков и корреляции наиболее значимых.
* Построить подходящие сводные таблицы и прочие диаграммы.
* Если проведенный вами анализ позволяет сделать какие-либо выводы, постараться сформулировать их максимально глубоко и развернуто. Упомянуть, как можно задействовать в дальнейшем приобретенные вами на этом этапе знания в решаемой задаче.
* Осуществить и другие осмысленные действия в рамках процесса EDA.


#### GitHub и требования при использовании СКВ
*   Для выполнения задания вы должны создать проект на GitHub (один репозиторий на всю команду!)
*   Необходимо добавить в репозиторий `.gitignore` и не грузить в репозиторий лишние файлы
*   Необходимо добавить в репозиторий `README.md` с подробным описанием проекта. В этот файл также необходимо добавить подробное описание данных — описание признаков, их тип, количество объектов каждого класса (в задачах классификации), информацию о пропущенных значениях (в т.ч о типичных филлерах, если это не `nan`). Также добавляйте другие релевантные вашей задачи описательные характеристики данных. После чтения этого файла у человека со стороны не должно остаться никаких вопросов про данные
*   В репозитории проекта размещаются ноутбуки / скрипты со сбором данных
*   Коммиты делаются осмысленно, с понятными внешнему наблюдателю комментариями — что было сделано по факту
*   При разработке необходимо соблюдать [гитфлоу](https://www.umgum.com/gitlab-workflow)

#### Артефакты по окончании работы над проектом
* Ноутбуки / скрипты со сбором и извлечением данных
* Ноутбуки c осуществлением EDA
* Файл `.gitignore`
* Заполненный файл `README.md` в репозитории проекта на гитхабе со ссылкой на данные и их подробным описанием, с выводами о проведенном EDA

#### Логирование
* Для получения высоких баллов добавьте в ваш проект логирование. Потренируйтесь включать и выключать его через файл настроек, а также задавать уровень логирования через этот же файл (не в приложении)
* Запустите ваш код, чтобы сохранился файл с логами
* Про логи можно почитать [тут](https://habr.com/ru/companies/wunderfund/articles/683880/) и [тут](https://tproger.ru/articles/shpargalka-po-logirovaniju-na-python)

#### Библиотека Selenium
* Для получения высоких баллов используйте библиотеку Selenium при сборе данных
* Реализуйте имитацию работы пользователя через Selenium
* Про данную библиотеку можно почитать [тут](https://habr.com/ru/companies/selectel/articles/754674/) и [тут](https://happypython.ru/2022/11/27/selenium-chromedriver-find-elements/)

## Критерии оценки проекта
Оценка за групповой проект в целом — то есть первая из двух оценок — выставляется по 10 балльной шкале, исходя из 3 критериев. Также комиссия на своё усмотрение может добавить (но не снизить) некоторое количество бонусных баллов за рамками данных критериев.

Каждый из критериев будет перечислен и рассмотрен ниже.

### Критерий 1. Выбор задачи и EDA (до 3,5 баллов)

Данный критерий оценивает, насколько обоснованной и осмысленной получилась решаемая бизнес-задача; насколько полно был осуществлен переход от бизнес-задачи к пайплайну подготовки данных (насколько в реальности формируемый датасет соответствует исходной постановке задачи с точки зрения интересов компании и насколько может помочь в решении такой задачи); а также насколько корректными, объёмными и прикладными для бизнеса получились выявленные в ходе исследования инсайты.

### Критерий 2. GitHub (до 1 балла)

Данный критерий оценивает, насколько корректным и квалифицированным было применение средств системы контроля версий в проекте; насколько были выполнены представленные в задании требования по всем необходимым файлам и условиям в рамках работы с GitHub; а также насколько эффективно данная система в действительности использовалась командой для распараллеливания задач по проекту.

### Критерий 3. Инструменты извлечения данных (до 5,5 баллов)

Данный критерий оценивает техническую реализацию процесса сбора данных автоматизированным образом с сайтов и порталов. В рамках этого критерия будет рассматриваться спектр различных технических инструментов, внедрённых в проект; корректность, эффективность и правильность их применения; а также сложность и новизна алгоритмов, подходов и библиотек, применяемых к данным.

*__Важно:__ в случае оценки __1 балл и ниже__ за этот критерий — оценка за весь проект становится равна оценке за этот критерий.*

*По данному критерию можно получить __больше 1 балла__ только при условии применения как средств Data Scraping (автоматического считывания HTML-кода страниц с последующей его обработкой), так и средств API (получению данных с использованием официальных протоколов компании/портала). При этом средства Scraping и API следует применять к разным компаниям/порталам, но представленным в рамках одной и той же предметной области (бизнес-задачи).*

*По данному критерию можно получить **3 балла и выше** только при условии использования в проекте логирования __ИЛИ__ библиотеки Selenium.*

*По данному критерию можно получить **4 балла и выше** только при условии использования в проекте логирования __И__ библиотеки Selenium.*

*По данному критерию можно получить **5 баллов и выше** только при условии использования в проекте логирования, библиотеки Selenium и, кроме этого, обоснованного применения дополнительных инструментов/подходов/библиотек, не упомянутых ранее в этом ноутбуке и не рассматривавшихся в рамках соответствующего семинара.*